{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/raul/Escritorio/busquedas_tid/.venv/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/raul/Escritorio/busquedas_tid/.venv/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/raul/Escritorio/busquedas_tid/.venv/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/raul/Escritorio/busquedas_tid/.venv/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/raul/Escritorio/busquedas_tid/.venv/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: beautifulsoup4 in /home/raul/Escritorio/busquedas_tid/.venv/lib/python3.12/site-packages (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/raul/Escritorio/busquedas_tid/.venv/lib/python3.12/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/raul/Escritorio/busquedas_tid/.venv/lib/python3.12/site-packages (from beautifulsoup4) (4.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /home/raul/Escritorio/busquedas_tid/.venv/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/raul/Escritorio/busquedas_tid/.venv/lib/python3.12/site-packages (from scikit-learn) (2.2.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/raul/Escritorio/busquedas_tid/.venv/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/raul/Escritorio/busquedas_tid/.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/raul/Escritorio/busquedas_tid/.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "%pip install beautifulsoup4\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/raul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/raul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/raul/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import collections\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all html files in path and transform in text\n",
    "#   If you get an UnicodeDecodeError remove file content in 'file' varaible\n",
    "path = './html'\n",
    "translate_table = dict((ord(char), ' ') for char in string.punctuation)\n",
    "token_dict = {}\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_path = subdir + os.path.sep + file\n",
    "        shakes = open(file_path, 'r')\n",
    "        try:\n",
    "          html = shakes.read()\n",
    "        except Exception:\n",
    "          continue\n",
    "        # Extract text from html\n",
    "        text = BeautifulSoup(html).get_text().encode('ascii', 'ignore')\n",
    "        # Lowercase and remove punctuation\n",
    "        lowers = str(text.lower())\n",
    "        no_punctuation = lowers.translate(translate_table)\n",
    "        token_dict[file] = no_punctuation\n",
    "\n",
    "# Show number of reading files\n",
    "len(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raul/Escritorio/busquedas_tid/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/raul/Escritorio/busquedas_tid/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['becau'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 11325 stored elements and shape (111, 561)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer function\n",
    "def process_text(text, stem=True):\n",
    "    \"\"\"Tokenize text and stem words removing punctuation\"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Transform texts to Tf-Idf coordinates\n",
    "# (http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "#  If you get an \"UserWarning: Your stop_words may be inconsistent\" ignore it\n",
    "stop_words = [process_text(w)[0] for w in stopwords.words('english')]\n",
    "vectorizer = TfidfVectorizer(tokenizer=process_text,\n",
    "                             stop_words=stop_words,\n",
    "                             max_df=0.5,\n",
    "                             min_df=0.1,\n",
    "                             lowercase=True)\n",
    "tfidf_model = vectorizer.fit_transform(token_dict.values())\n",
    "\n",
    "# Show dimensions of tfidf_model\n",
    "tfidf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '000', '1', '10', '100', '11', '12', '13', '14', '15', '16',\n",
       "       '17', '18', '19', '2', '20', '2023', '2024', '2026', '21', '22',\n",
       "       '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32',\n",
       "       '35', '36', '4', '40', '49', '5', '50', '500', '6', '7', '8', '9',\n",
       "       'access', 'account', 'achiev', 'across', 'action', 'activ', 'ad',\n",
       "       'addit', 'address', 'advanc', 'age', 'agre', 'allow', 'also',\n",
       "       'alway', 'america', 'american', 'angel', 'announc', 'annual',\n",
       "       'app', 'appli', 'applic', 'april', 'around', 'articl', 'assist',\n",
       "       'athlet', 'avail', 'award', 'back', 'base', 'basketbal', 'becom',\n",
       "       'begin', 'benefit', 'best', 'better', 'bodi', 'book', 'box',\n",
       "       'break', 'bring', 'build', 'busi', 'c', 'call', 'camp', 'canada',\n",
       "       'care', 'career', 'celebr', 'center', 'central', 'challeng',\n",
       "       'champion', 'championship', 'chang', 'check', 'choic', 'choos',\n",
       "       'citi', 'class', 'click', 'close', 'club', 'coach', 'code',\n",
       "       'collect', 'com', 'combin', 'come', 'committe', 'common', 'commun',\n",
       "       'compet', 'competit', 'compil', 'complet', 'comput', 'condit',\n",
       "       'connect', 'contact', 'contain', 'content', 'continu', 'contribut',\n",
       "       'control', 'cooki', 'copyright', 'countri', 'cours', 'creat',\n",
       "       'cup', 'current', 'data', 'date', 'day', 'de', 'design', 'detail',\n",
       "       'develop', 'differ', 'director', 'disabl', 'discov', 'document',\n",
       "       'donat', 'download', 'e', 'earn', 'easi', 'edit', 'editor', 'educ',\n",
       "       'els', 'email', 'en', 'enabl', 'end', 'ensur', 'enter', 'equiti',\n",
       "       'error', 'even', 'event', 'everi', 'exampl', 'excel', 'except',\n",
       "       'exclus', 'exist', 'experi', 'explor', 'extens', 'facebook',\n",
       "       'fame', 'famili', 'faq', 'fast', 'featur', 'feder', 'field',\n",
       "       'file', 'final', 'financi', 'find', 'finish', 'first', 'five',\n",
       "       'follow', 'form', 'forward', 'found', 'foundat', 'four', 'free',\n",
       "       'friend', 'full', 'function', 'futur', 'game', 'gener', 'get',\n",
       "       'give', 'global', 'go', 'goal', 'gold', 'govern', 'great', 'group',\n",
       "       'grow', 'guid', 'hall', 'happen', 'head', 'health', 'help', 'high',\n",
       "       'histor', 'histori', 'home', 'host', 'http', 'impact', 'import',\n",
       "       'improv', 'includ', 'inclus', 'individu', 'inform', 'initi',\n",
       "       'inspir', 'integr', 'interest', 'intern', 'introduc', 'involv',\n",
       "       'issu', 'javascript', 'job', 'join', 'journey', 'jump', 'junior',\n",
       "       'know', 'la', 'languag', 'last', 'latest', 'lead', 'leader',\n",
       "       'leadership', 'leagu', 'learn', 'legal', 'level', 'librari',\n",
       "       'licens', 'life', 'like', 'line', 'link', 'list', 'live', 'lo',\n",
       "       'local', 'locat', 'log', 'login', 'long', 'look', 'made', 'mail',\n",
       "       'main', 'major', 'make', 'manag', 'mani', 'mar', 'march', 'master',\n",
       "       'may', 'mean', 'medal', 'media', 'meet', 'member', 'membership',\n",
       "       'memori', 'men', 'menu', 'million', 'mission', 'model', 'month',\n",
       "       'much', 'n2', 'n2025', 'n4', 'na', 'nabout', 'nall', 'name',\n",
       "       'nathlet', 'nation', 'navig', 'nbecom', 'nbi', 'nc', 'ncareer',\n",
       "       'nclose', 'ncode', 'ncontact', 'ncooki', 'ncopyright', 'ndonat',\n",
       "       'near', 'network', 'nevent', 'news', 'newslett', 'next',\n",
       "       'nfacebook', 'nfaq', 'nfeatur', 'nfind', 'nfollow', 'nfor', 'nget',\n",
       "       'nhome', 'nhow', 'ni', 'nif', 'nin', 'ninstagram', 'njoin',\n",
       "       'nlatest', 'nlearn', 'nmedia', 'nmore', 'nnation', 'nnew',\n",
       "       'noffici', 'non', 'nopen', 'note', 'nour', 'npartner', 'nprivaci',\n",
       "       'nprogram', 'nread', 'nreport', 'nresourc', 'nsearch', 'nsee',\n",
       "       'nshop', 'nsign', 'nskip', 'nspecial', 'nsport', 'nstart',\n",
       "       'nsupport', 'nteam', 'nterm', 'nthe', 'nthi', 'ntwitter', 'nu',\n",
       "       'number', 'nusa', 'nvideo', 'nview', 'nwe', 'nwhat', 'nyour',\n",
       "       'nyoutub', 'offer', 'offici', 'olymp', 'one', 'onlin', 'open',\n",
       "       'oper', 'opportun', 'option', 'org', 'organ', 'p', 'page', 'para',\n",
       "       'paralymp', 'part', 'particip', 'partner', 'partnership', 'peopl',\n",
       "       'perform', 'person', 'place', 'plan', 'platform', 'play', 'player',\n",
       "       'pleas', 'plu', 'point', 'polici', 'possibl', 'potenti', 'power',\n",
       "       'practic', 'pre', 'prefer', 'present', 'privaci', 'pro', 'process',\n",
       "       'product', 'profession', 'programm', 'project', 'promot',\n",
       "       'protect', 'provid', 'public', 'purpos', 'qualifi', 'que',\n",
       "       'question', 'rang', 'reach', 'read', 'readi', 'real', 'receiv',\n",
       "       'record', 'refer', 'region', 'regist', 'relat', 'releas', 'report',\n",
       "       'repres', 'request', 'requir', 'reserv', 'resourc', 'respons',\n",
       "       'result', 'return', 'review', 'right', 'road', 'role', 'round',\n",
       "       'rule', 'run', 'safeti', 'schedul', 'school', 'scienc', 'score',\n",
       "       'search', 'season', 'second', 'secur', 'see', 'select', 'seri',\n",
       "       'servic', 'set', 'share', 'shop', 'show', 'sign', 'sinc', 'site',\n",
       "       'skate', 'skill', 'skip', 'social', 'softwar', 'sourc', 'special',\n",
       "       'speed', 'sponsor', 'sport', 'staff', 'stage', 'standard', 'star',\n",
       "       'start', 'state', 'statement', 'step', 'store', 'stori',\n",
       "       'structur', 'student', 'style', 'success', 'suggest', 'summer',\n",
       "       'support', 'system', 'take', 'team', 'technolog', 'term', 'test',\n",
       "       'three', 'time', 'titl', 'today', 'togeth', 'tool', 'top', 'topic',\n",
       "       'track', 'train', 'tri', 'two', 'type', 'u', 'unifi', 'unit',\n",
       "       'univers', 'updat', 'us', 'user', 'valu', 'version', 'video',\n",
       "       'view', 'volunt', 'want', 'watch', 'way', 'web', 'websit', 'week',\n",
       "       'weekend', 'welcom', 'well', 'wide', 'win', 'window', 'winter',\n",
       "       'within', 'without', 'women', 'work', 'write', 'x', 'year'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show selected tokens\n",
    "#   On old versions of scikit-learn change 'get_feature_names_out' method by\n",
    "#   'get_feature_names'\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 30\n",
      "0: 69\n",
      "1: 12\n",
      "2: https://www.usatriathlon.org/\n",
      "0: https://www.mbusa.com/en/home\n",
      "2: https://www.usaartisticswim.org/\n",
      "1: https://es.wikipedia.org/wiki/C%2B%2B\n",
      "0: https://cplusplus.com/\n",
      "0: https://www.open-std.org/jtc1/sc22/wg21/\n",
      "2: http://www.ussoccer.com/\n",
      "0: https://la28.org/\n",
      "1: https://computerhoy.20minutos.es/tecnologia/bjarne-stroustrup-aprender-programar-1445288\n",
      "0: https://2026specialolympicsusagames.org/\n",
      "0: https://educationusa.state.gov/\n",
      "2: https://www.usab.com/\n",
      "0: https://wiki.sei.cmu.edu/confluence/pages/viewpage.action?pageId=88046682\n",
      "0: https://www.usaswimming.org/\n",
      "0: https://www.codecademy.com/learn/learn-c-plus-plus\n",
      "0: https://www.w3schools.com/CPP/default.asp\n",
      "1: https://visualstudio.microsoft.com/es/vs/features/cplusplus/\n",
      "0: https://www.specialolympics.org/\n",
      "0: https://www.usa829.org/\n",
      "2: https://www.udacity.com/course/c-plus-plus-nanodegree--nd213\n",
      "2: https://www.unicefusa.org/\n",
      "0: https://en.wikipedia.org/wiki/Summer/Olympic/Games\n",
      "2: https://usrowing.org/teams/olympic\n",
      "1: https://www.abc.es/familia/padres-hijos/psicologa-comparte-estrategias-mejorar-autoestima-pequenos-pacientes-20250402151724-nt\n",
      "0: https://www.specialolympics-ny.org/\n",
      "0: https://usafootball.com/\n",
      "0: https://soindiana.org/\n",
      "0: https://ccusa.com/\n",
      "0: https://www.usahockeyntdp.com/\n",
      "2: https://usasouth.net/\n",
      "0: https://specialolympicspa.org/\n",
      "0: https://www.specialolympicsgb.org.uk/\n",
      "0: https://usafacts.org/\n",
      "2: https://twitter.com/NBCOlympics\n",
      "1: https://www.facebook.com/olympics/\n",
      "0: https://www.netacad.com/courses/c-plus-plus-essentials-1\n",
      "0: https://roadmap.sh/cpp\n",
      "0: https://www.vestas.com/en/media/company-news/2025/vestas-wins-172-mw-order-in-the-usa-c4128541\n",
      "2: https://www.usopc.org/careers\n",
      "0: https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines\n",
      "2: https://isocpp.org/about\n",
      "1: https://www.usa.gov/es/\n",
      "0: https://www.nbcolympics.com/\n",
      "0: https://en.wikipedia.org/wiki/United/States/at/the/Olympics\n",
      "2: https://www.teamusa.com/\n",
      "2: https://app.santanderopenacademy.com/es/program/usa-summer-experience-2025-penn\n",
      "1: https://as.com/actualidad/sociedad/una-mujer-usa-una-roca-como-sujeta-puertas-durante-decadas-y-resulta-que-es-un-material-mucho-mas-valioso-n/\n",
      "2: https://www.usopc.org/about-the-usopc\n",
      "0: https://www.girlscouts.org/\n",
      "0: https://www.mazdausa.com/\n",
      "0: https://www.bbc.co.uk/sport/olympics\n",
      "0: https://marketplace.visualstudio.com/items?itemName=ms-vscode.cpptools\n",
      "0: https://isocpp.org/\n",
      "0: https://www.boost.org/\n",
      "0: https://www.bmwusa.com/\n",
      "2: https://usawaterpolo.org/\n",
      "0: https://usagym.org/\n",
      "0: https://www.usa.gov/\n",
      "0: https://www.techtarget.com/searchdatamanagement/definition/C\n",
      "0: https://www.specialolympics.org/about\n",
      "0: https://cppnow.org/\n",
      "0: https://www.programiz.com/cpp-programming\n",
      "2: https://www.us.hsbc.com/\n",
      "0: https://www.ymca.org/\n",
      "0: https://usacycling.org/\n",
      "0: https://www.learncpp.com/\n",
      "2: https://team26.milanocortina2026.org/\n",
      "0: https://soill.org/\n",
      "0: https://usavolleyball.org/\n",
      "2: https://www.usawaterski.org/\n",
      "0: https://en.wikipedia.org/wiki/Olympic/Games\n",
      "0: https://www.espn.com/olympics/\n",
      "0: https://www.theregister.com/2024/11/08/the/us/government/wants/developers/\n",
      "2: https://www.usaboxing.org/\n",
      "0: https://www.skillsusa.org/\n",
      "0: https://www.usarchery.org/\n",
      "0: https://www.specialolympics.ca/\n",
      "0: https://www.jetbrains.com/clion/\n",
      "2: https://www.teamusa.com/athletes\n",
      "0: https://www.gov.uk/foreign-travel-advice/usa\n",
      "1: https://www.auswaertiges-amt.de/de/reiseundsicherheit/usavereinigtestaatensicherheit-201382\n",
      "0: https://www.embarcadero.com/products/cbuilder\n",
      "0: https://www.bloodshed.net/\n",
      "2: https://www.hackerrank.com/domains/cpp\n",
      "2: https://www.usab.com/teams/5x5-mens-olympics/roster\n",
      "0: https://www.usacurling.org/\n",
      "2: https://usaco.org/\n",
      "1: https://www.instagram.com/olympics/\n",
      "0: https://www.usalacrosse.com/\n",
      "1: https://www.instagram.com/teamusa/\n",
      "0: https://www.w3schools.com/cpp/cpp/intro.asp\n",
      "0: https://learn.microsoft.com/en-us/cpp/?view=msvc-170\n",
      "0: https://www.miniusa.com/content/miniusa/en\n",
      "2: https://x.com/teamusa\n",
      "0: https://developer.mozilla.org/en-US/docs/WebAssembly/Guides/C/to/Wasm\n",
      "1: https://as.com/us/videos/el-impresionante-doble-de-lamine-yamal-que-rompe-las-redes-v/\n",
      "0: https://www.stroustrup.com/\n",
      "0: https://usa.rugby/\n",
      "2: https://www.usadiving.org/\n",
      "2: https://www.usaweightlifting.org/\n",
      "2: https://www.nissanusa.com/\n",
      "0: https://www.usahockey.com/\n",
      "1: https://www.malagahoy.es/malaga/que-es-tramadol-usos-cuidados/0/2003642380\n",
      "0: https://www.usatoday.com/sports/olympics/\n",
      "0: https://www.cbc.ca/sports/olympics\n",
      "2: https://x.com/olympics\n",
      "2: https://www.volvocars.com/\n",
      "0: https://usatf.org/\n",
      "0: https://code.visualstudio.com/docs/languages/cpp\n",
      "0: https://freepressokc.com/oklahoma-city-chosen-to-host-two-2028-olympic-sports/\n",
      "2: https://www.usa.canon.com/?srsltid=AfmBOooA0anOwZhYyPCuwSJdU7NHgnHfXinQIY0nVAcxIBI0CtF2o21K\n",
      "2:\n",
      "   c  -  0.13916664559460878\n",
      "   olymp  -  0.1113391719375293\n",
      "   game  -  0.06637811218127115\n",
      "   team  -  0.057843193843278636\n",
      "\n",
      "0:\n",
      "   de  -  0.6873114692368595\n",
      "   en  -  0.3346052164708951\n",
      "   la  -  0.28617465886250826\n",
      "   que  -  0.228163733821036\n",
      "\n",
      "1:\n",
      "   open  -  0.2929546797768483\n",
      "   window  -  0.05285626178147688\n",
      "   nation  -  0.04826085066900108\n",
      "   olymp  -  0.04536233852317797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cluster texts using K-Means\n",
    "# (http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "#   On old versions of scikit-learn remove 'n_init' parameter\n",
    "km_model = KMeans(n_clusters=3, n_init='auto', verbose=0)\n",
    "km_model.fit(tfidf_model)\n",
    "\n",
    "# Print clusters\n",
    "clusters = collections.defaultdict(list)\n",
    "for idx, label in enumerate(km_model.labels_):\n",
    "    clusters[label].append(idx)\n",
    "\n",
    "dict(clusters)\n",
    "\n",
    "# Print number of elements of each cluster\n",
    "for key, elements in dict(clusters).items():\n",
    "    print(str(key) + ':', len(elements))\n",
    "\n",
    "# Print labels of each url webpage\n",
    "key = list(token_dict.keys())\n",
    "for idx, label in enumerate(km_model.labels_):\n",
    "    print(str(label) + ':', key[idx].replace('_','/').replace('.html',''))\n",
    "\n",
    "# Print 4 most relevant tokens of each cluster\n",
    "kmcc = km_model.cluster_centers_.copy()\n",
    "for idx, item in enumerate(dict(clusters).items()):\n",
    "    print(str(item[0]) + ':')\n",
    "    for j in range(4):\n",
    "        idxmax = kmcc[idx].argmax()\n",
    "        print('  ', feature_names[idxmax], ' - ', kmcc[idx][idxmax])\n",
    "        kmcc[idx][idxmax] = 0.0\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
